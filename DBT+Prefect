# Industrial Case Study: Data Transformation & Workflow Orchestration for a Global Retail Chain

## Title
Optimizing Data Transformation Pipelines Using Prefect and DBT for a Global Retail Store

## Industry Context
Global Super Store is a large multinational retail chain operating across several countries. With increasing global operations, they collect massive amounts of transactional, shipping, and customer data. However, the business struggles to efficiently transform raw sales data into actionable insights due to:

- Manual or inconsistent transformation processes
- Lack of visibility into workflow execution and failures
- No scheduling or orchestration of data transformation tasks
- Poor collaboration between data engineering and analytics teams

## Problem Statement
The organization wants to streamline its ETL pipeline using modern, open-source tools. The goal is to ensure reproducibility, transparency, and scalability in how raw data is ingested, transformed, and made analytics-ready.

## Objective
Build a modern data pipeline using:
- **DBT (Data Build Tool)** for transforming raw data into analytics-ready models using SQL
- **Prefect (Open Source)** for orchestrating the workflow: ingestion, transformation, and monitoring

## Dataset
We will use the open-source [Global Super Store dataset from Kaggle](https://www.kaggle.com/datasets/apoorvaappz/global-super-store-dataset), which includes:
- Orders and returns data
- Shipping, regional, and product information

## Key Tasks
1. **Ingest** the CSV files into a local/temporary SQLite or Postgres database.
2. **Transform** the data using DBT into fact and dimension models.
3. **Orchestrate** the workflow using Prefect with task-level logging and success/failure notifications.
4. **Document and visualize** the DAG and transformations.

## Business Outcome
Enable near-real-time, reliable, and reproducible analytics-ready data for:
- Sales trend analysis
- Regional performance tracking
- Return rate insights
- Shipping delays and inefficiencies

## Tools & Tech Stack
- **Python** for scripting and task execution
- **DBT Core** for SQL-based transformation and testing
- **Prefect Open Source** for orchestration
- **SQLite or PostgreSQL** as target warehouse
